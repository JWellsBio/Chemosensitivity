## test ROSE package
if (!require ('ROSE')) install.packages('ROSE')
library(ROSE)

if (!require('dplyr')) install.packages('dplyr')
library(dplyr)

if (!require('ggplot2')) install.packages('ggplot2')
library(ggplot2)

if (!require(earth)) install.packages('earth')
library(earth)

if (!require('caret')) install.packages('caret')
library(caret)

if (!require('vip')) install.packages('vip')
library(vip)

if (!require('pdp')) install.pacakges('pdp') 
library(pdp)

if (!require('kernlab')) install.packages('kernlab')
library(kernlab)

if (!require('nnet')) install.packages('nnet')
library(nnet)

if (!require('quantreg')) install.packages('quantreg')
library(quantreg)

# 2-dimensional example
# loading data
data(hacide)
# imbalance on training set
table(hacide.train$cls)
#imbalance on test set
table(hacide.test$cls)

# plot unbalanced data highlighting the majority and
# minority class examples.
par(mfrow=c(1,2))
plot(hacide.train[, 2:3], main="Unbalanced data", xlim=c(-4,4),
     ylim=c(-4,4), col=as.numeric(hacide.train$cls), pch=20)
legend("topleft", c("Majority class","Minority class"), pch=20, col=1:2)
# model estimation using logistic regression
fit <- glm(cls~., data=hacide.train, family="binomial")
# prediction using test set
pred <- predict(fit, newdata=hacide.test)
roc.curve(hacide.test$cls, pred,
          main="ROC curve \n (Half circle depleted data)")
# generating data according to ROSE: p=0.5 as default
data.rose <- ROSE(cls~., data=hacide.train, seed=3)$data
table(data.rose$cls)
par(mfrow=c(1,2))
# plot new data generated by ROSE highlighting the
# majority and minority class examples.
plot(data.rose[, 2:3], main="Balanced data by ROSE",
     xlim=c(-6,6), ylim=c(-6,6), col=as.numeric(data.rose$cls), pch=20)
legend("topleft", c("Majority class","Minority class"), pch=20, col=1:2)
fit.rose <- glm(cls~., data=data.rose, family="binomial")
pred.rose <- predict(fit.rose, data=data.rose, type="response")
roc.curve(data.rose$cls, pred.rose,
          main="ROC curve \n (Half circle depleted data balanced by ROSE)")
par(mfrow=c(1,1))








#trying multivariate adaptive regression splines


bleomycin_rose_df <- bleomycin_rose
bleomycin_rose_df$res_sens <- bleomycin_rose_res_sens



# fit a basic MARS model
mars1 <- earth(res_sens ~ ., data = bleomycin_rose_df)
#print model summary
print(mars1)
summary(mars1)
coef(mars1)

#plot it ###THIS TAKES FOREVER
plot(mars1, which = 1)

# fit a basic MARS model
mars2 <- earth(res_sens ~ ., data = bleomycin_rose_df, degree = 2)

summary(mars2)


## create a tuning grid
hyper_grid <- expand.grid(degree = 1:3, nprune = seq(2,100,length.out = 10) %>% floor())

head(hyper_grid)

# cross-validated model
set.seed(5)
cv_mars <- train(x = subset(bleomycin_rose_df, select = -res_sens), 
                 y = as.factor(bleomycin_rose_df$res_sens), 
                 method = 'earth', 
                 trControl = trainControl(method = 'cv', number = 10), 
                 tuneGrid = hyper_grid)

#results
cv_mars$bestTune

#plot it
ggplot(cv_mars)

#refine grid search (nprune)

## feature importance
# this should be done on cv_mars
p1 <- vip(cv_mars, num_features = 23, bar = FALSE, value = 'gcv') + ggtitle('GCV')
p1
p2 <- vip(cv_mars, num_features = 23, bar = FALSE, value = 'rss') + ggtitle('RSS')
p2
#partial dependence plots (although these don't make a ton of sense in binary classification)
p1 <- partial(mars2, pred.var = 'ENSG00000105388', grid.resolution = 10) %>% autoplot()
p2 <- partial(mars2, pred.var = 'ENSG00000122133', grid.resolution = 10) %>% autoplot()





#comparing multiple methods
## MARS
#data
data(longley)
#fit model
fit <- earth(res_sens ~ ., bleomycin_rose_df)
#summarize
summary(fit)
# summarize importance of input vars
evimp(fit)
# make predictions
predictions <- predict(fit, bleomycin_rose_df)
# summarize accuracy
mse <- mean((bleomycin_rose_df$res_sens - predictions) ^ 2)
mse #0.022



## SVM
data(longley)
# fit model
fit <- ksvm(res_sens ~ ., bleomycin_rose_df)
summary(fit) # is there better info than this?

predictions <- predict(fit, bleomycin_rose_df)
mse <- mean((bleomycin_rose_df$res_sens - predictions) ^ 2)
mse #0.003




## kNN
fit <- knnreg(bleomycin_rose_df[ ,1:14209], bleomycin_rose_df[, 14210], k = 2)
summary(fit)
predictions <- predict(fit, bleomycin_rose_df[, 1:14209])
mse <- mean((bleomycin_rose_df$res_sens - predictions) ^ 2)
mse #0.013



# neural net
data(longley)
x <- bleomycin_rose_df[, 1:14209]
y <- bleomycin_rose_df[, 14210]
fit <- nnet(res_sens ~ ., bleomycin_rose_df, size = 12, maxit = 500, linout = T, decay = 0.01)
summary(fit)
predictions <- predict(fit, x, type = 'raw')
mse <- mean((y - predictions) ^ 2)
mse #0.00002
